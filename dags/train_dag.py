"""
Airflow DAG cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh
Sá»­ dá»¥ng RabbitMQ Ä‘á»ƒ giao tiáº¿p giá»¯a cÃ¡c service
Kiáº¿n trÃºc:
- MÃ¡y Airflow (localhost): Cháº¡y Airflow, RabbitMQ
- MÃ¡y Hadoop (192.168.80.127): Cháº¡y HDFS
- MÃ¡y Spark (192.168.80.207): Cháº¡y Spark
"""
from airflow import DAG
try:
    from airflow.providers.standard.operators.bash import BashOperator
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    from airflow.operators.bash import BashOperator
    from airflow.operators.python import PythonOperator

from datetime import datetime, timedelta
import time
import socket
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.rabbitmq_client import get_rabbitmq_client

# ========================================
# Cáº¤U HÃŒNH Há»† THá»NG PHÃ‚N TÃN
# ========================================
# Theo CLUSTER_NODES trong system_worker.py
HADOOP_HOST = "192.168.80.127"  # hadoop-namenode, hadoop-datanode
SPARK_HOST, SPARK_USER = "192.168.80.207", "nindang"  # spark-master, spark-worker
SPARK_MASTER = f"spark://{SPARK_HOST}:7077"
PROJECT_DIR = "/home/haminhchien/Documents/bigdata/final_project"

# RabbitMQ cháº¡y trÃªn mÃ¡y Airflow (cÃ¹ng mÃ¡y) - khÃ´ng cáº§n IP, dÃ¹ng localhost
RABBITMQ_HOST = "localhost"  # hoáº·c "127.0.0.1" - cÃ¹ng mÃ¡y nÃªn khÃ´ng cáº§n IP
RABBITMQ_PORT = 5672

# Cáº¥u hÃ¬nh HDFS
HDFS_NAMENODE = "hdfs://192.168.80.127:9000"
HDFS_DATA_DIR = "/bigdata/house_prices"
HDFS_MODEL_DIR = "/bigdata/house_prices/models"

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def check_service_ready(host, port, name, max_retries=10, delay=5):
    """Kiá»ƒm tra service Ä‘Ã£ sáºµn sÃ ng"""
    for i in range(max_retries):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                if sock.connect_ex((host, port)) == 0:
                    print(f"âœ“ {name} Ä‘Ã£ sáºµn sÃ ng táº¡i {host}:{port}!")
                    return True
            print(f"â³ Chá» {name}... (láº§n {i+1}/{max_retries})")
        except Exception as e:
            print(f"âŒ Lá»—i khi kiá»ƒm tra {name}: {e}")
        time.sleep(delay)
    raise Exception(f"{name} khÃ´ng sáºµn sÃ ng táº¡i {host}:{port}")

def check_hadoop_ready(**_):
    """Kiá»ƒm tra Hadoop HDFS Ä‘Ã£ sáºµn sÃ ng"""
    return check_service_ready(HADOOP_HOST, 9000, "Hadoop NameNode", max_retries=20, delay=10)

def check_spark_ready(**_):
    """Kiá»ƒm tra Spark Ä‘Ã£ sáºµn sÃ ng"""
    return check_service_ready(SPARK_HOST, 7077, "Spark Master", max_retries=20, delay=10)

def check_rabbitmq_ready(**_):
    """Kiá»ƒm tra RabbitMQ Ä‘Ã£ sáºµn sÃ ng"""
    return check_service_ready(RABBITMQ_HOST, RABBITMQ_PORT, "RabbitMQ", max_retries=10, delay=5)

def send_training_start_message(**_):
    """Gá»­i message báº¯t Ä‘áº§u training qua RabbitMQ"""
    try:
        # RabbitMQ cháº¡y trÃªn cÃ¹ng mÃ¡y nÃªn khÃ´ng cáº§n truyá»n host/port (dÃ¹ng default localhost)
        client = get_rabbitmq_client()
        client.connect()
        client.publish_message(
            queue_name='training_status',
            message={
                'status': 'started',
                'timestamp': datetime.now().isoformat(),
                'stage': 'training'
            }
        )
        client.close()
        print("âœ“ ÄÃ£ gá»­i message báº¯t Ä‘áº§u training")
    except Exception as e:
        print(f"âš ï¸  KhÃ´ng thá»ƒ gá»­i message: {e}")

def send_training_complete_message(**_):
    """Gá»­i message hoÃ n thÃ nh training qua RabbitMQ"""
    try:
        # RabbitMQ cháº¡y trÃªn cÃ¹ng mÃ¡y nÃªn khÃ´ng cáº§n truyá»n host/port (dÃ¹ng default localhost)
        client = get_rabbitmq_client()
        client.connect()
        client.publish_message(
            queue_name='training_status',
            message={
                'status': 'completed',
                'timestamp': datetime.now().isoformat(),
                'stage': 'training',
                'model_path': f"{HDFS_MODEL_DIR}/house_price_model"
            }
        )
        client.close()
        print("âœ“ ÄÃ£ gá»­i message hoÃ n thÃ nh training")
    except Exception as e:
        print(f"âš ï¸  KhÃ´ng thá»ƒ gá»­i message: {e}")

# ========================================
# DAG: TRAIN MODEL
# ========================================
with DAG(
    'train_model_pipeline',
    default_args=default_args,
    description='Train model pipeline vá»›i HDFS vÃ  RabbitMQ',
    schedule=None,
    catchup=False,
    tags=['training', 'hdfs', 'spark', 'rabbitmq'],
) as dag:
    
    # Task 1: Kiá»ƒm tra cÃ¡c service sáºµn sÃ ng
    check_rabbitmq = PythonOperator(
        task_id='check_rabbitmq',
        python_callable=check_rabbitmq_ready,
    )
    
    check_hadoop = PythonOperator(
        task_id='check_hadoop',
        python_callable=check_hadoop_ready,
    )
    
    check_spark = PythonOperator(
        task_id='check_spark',
        python_callable=check_spark_ready,
    )
    
    # Task 2: Chuáº©n bá»‹ dá»¯ liá»‡u local
    prepare_data = BashOperator(
        task_id='prepare_data',
        bash_command=f"""
        cd {PROJECT_DIR}
        if [ -f data/train_data.csv ]; then
            echo "âœ“ Dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn"
        else
            echo "ðŸ“Š Äang chuáº©n bá»‹ dá»¯ liá»‡u..."
            python data/prepare_data.py
        fi
        """,
    )
    
    # Task 3: Upload dá»¯ liá»‡u lÃªn HDFS
    upload_to_hdfs = BashOperator(
        task_id='upload_to_hdfs',
        bash_command=f"""
        cd {PROJECT_DIR}
        echo "ðŸ“¤ Äang upload dá»¯ liá»‡u lÃªn HDFS..."
        python data/upload_to_hdfs.py
        echo "âœ“ ÄÃ£ upload dá»¯ liá»‡u lÃªn HDFS"
        """,
    )
    
    # Task 4: Gá»­i message báº¯t Ä‘áº§u training
    notify_training_start = PythonOperator(
        task_id='notify_training_start',
        python_callable=send_training_start_message,
    )
    
    # Task 5: Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn Spark cluster
    train_model = BashOperator(
        task_id='train_model',
        bash_command=f"""
        cd {PROJECT_DIR}
        echo "ðŸš€ Gá»­i training job Ä‘áº¿n Spark: {SPARK_MASTER}"
        echo "HDFS Namenode: {HDFS_NAMENODE}"
        echo "HDFS Data Dir: {HDFS_DATA_DIR}"
        echo "HDFS Model Dir: {HDFS_MODEL_DIR}"
        
        spark-submit --master {SPARK_MASTER} \\
            --conf spark.hadoop.fs.defaultFS={HDFS_NAMENODE} \\
            --conf spark.local.dir=/tmp/spark_local \\
            --driver-memory 4g \\
            --executor-memory 4g \\
            --num-executors 2 \\
            --executor-cores 2 \\
            --conf spark.hadoop.fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem \\
            spark_jobs/train_model.py
        echo "âœ“ Training hoÃ n thÃ nh"
        """,
        env={
            'HDFS_NAMENODE': HDFS_NAMENODE,
            'HDFS_DATA_DIR': HDFS_DATA_DIR,
            'HDFS_MODEL_DIR': HDFS_MODEL_DIR,
        }
    )
    
    # Task 6: Gá»­i message hoÃ n thÃ nh training
    notify_training_complete = PythonOperator(
        task_id='notify_training_complete',
        python_callable=send_training_complete_message,
    )
    
    # Äá»‹nh nghÄ©a dependencies
    [check_rabbitmq, check_hadoop, check_spark] >> prepare_data >> upload_to_hdfs >> notify_training_start >> train_model >> notify_training_complete

