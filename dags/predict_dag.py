"""
Airflow DAG cho qu√° tr√¨nh d·ª± ƒëo√°n streaming
S·ª≠ d·ª•ng RabbitMQ/Celery ƒë·ªÉ ƒëi·ªÅu khi·ªÉn c√°c node t·ª´ xa (kh√¥ng d√πng SSH)
Ki·∫øn tr√∫c:
- M√°y Airflow (localhost): Ch·∫°y Airflow, RabbitMQ
- M√°y Kafka (192.168.80.127): Ch·∫°y Kafka (Celery worker queue: node_57)
- M√°y Spark (192.168.80.207): Ch·∫°y Spark (Celery worker queue: spark)
- M√°y Hadoop (192.168.80.127): Ch·∫°y HDFS
"""
from airflow import DAG
try:
    from airflow.providers.standard.operators.bash import BashOperator
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    from airflow.operators.bash import BashOperator
    from airflow.operators.python import PythonOperator

from datetime import datetime, timedelta
import time
import socket
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.rabbitmq_client import get_rabbitmq_client
from mycelery.system_worker import docker_compose_up, run_command

# ========================================
# C·∫§U H√åNH H·ªÜ TH·ªêNG PH√ÇN T√ÅN
# ========================================
# Theo CLUSTER_NODES trong system_worker.py
KAFKA_HOST, KAFKA_PORT = "192.168.80.127", 9092  # kafka node
SPARK_HOST = "192.168.80.207"  # spark-master, spark-worker
SPARK_MASTER = f"spark://{SPARK_HOST}:7077"
PROJECT_DIR = "/home/haminhchien/Documents/bigdata/final_project"

# RabbitMQ ch·∫°y tr√™n m√°y Airflow (c√πng m√°y) - kh√¥ng c·∫ßn IP, d√πng localhost
RABBITMQ_HOST = "localhost"  # ho·∫∑c "127.0.0.1" - c√πng m√°y n√™n kh√¥ng c·∫ßn IP
RABBITMQ_PORT = 5672

# Queue mapping (kh·ªõp v·ªõi CLUSTER_NODES trong system_worker.py)
KAFKA_QUEUE = "node_57"
SPARK_QUEUE = "spark"

# C·∫•u h√¨nh HDFS
HDFS_NAMENODE = "hdfs://192.168.80.127:9000"
HDFS_MODEL_DIR = "/bigdata/house_prices/models"

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


def check_service_ready(host, port, name, max_retries=10, delay=5):
    """Ki·ªÉm tra service TCP (Kafka/Spark/RabbitMQ) ƒë√£ s·∫µn s√†ng"""
    for i in range(max_retries):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                if sock.connect_ex((host, port)) == 0:
                    print(f"‚úì {name} ƒë√£ s·∫µn s√†ng t·∫°i {host}:{port}!")
                    return True
            print(f"‚è≥ Ch·ªù {name}... (l·∫ßn {i+1}/{max_retries})")
        except Exception as e:
            print(f"‚ùå L·ªói khi ki·ªÉm tra {name}: {e}")
        time.sleep(delay)
    raise Exception(f"{name} kh√¥ng s·∫µn s√†ng t·∫°i {host}:{port}")


def check_kafka_ready(**_):
    """Ki·ªÉm tra Kafka ƒë√£ s·∫µn s√†ng"""
    return check_service_ready(KAFKA_HOST, KAFKA_PORT, "Kafka", max_retries=30, delay=10)


def check_spark_ready(**_):
    """Ki·ªÉm tra Spark ƒë√£ s·∫µn s√†ng"""
    return check_service_ready(SPARK_HOST, 7077, "Spark Master", max_retries=20, delay=10)


def check_rabbitmq_ready(**_):
    """Ki·ªÉm tra RabbitMQ ƒë√£ s·∫µn s√†ng"""
    return check_service_ready(RABBITMQ_HOST, RABBITMQ_PORT, "RabbitMQ", max_retries=10, delay=5)


def wait_for_celery_result(result, timeout=600, poll_interval=5):
    """ƒê·ª£i Celery task ho√†n th√†nh qua RabbitMQ"""
    elapsed = 0
    while elapsed < timeout:
        if result.ready():
            if result.successful():
                return result.result
            raise Exception(f"Celery task failed: {result.result}")
        time.sleep(poll_interval)
        elapsed += poll_interval
    raise TimeoutError(f"Celery task {result.id} timed out sau {timeout} gi√¢y")


def start_kafka_via_celery(**context):
    """
    Kh·ªüi ƒë·ªông Kafka cluster tr√™n node Kafka th√¥ng qua Celery/RabbitMQ
    Gi·∫£ ƒë·ªãnh c√≥ docker-compose kafka tr√™n node, v√≠ d·ª•: ~/kafka-cluster/docker-compose.yml
    """
    compose_path = "~/kafka-cluster/docker-compose.yml"
    print(f"üöÄ G·ª≠i l·ªánh docker-compose up Kafka t·ªõi queue '{KAFKA_QUEUE}' qua RabbitMQ")

    result = docker_compose_up.apply_async(
        args=[compose_path],
        kwargs={
            "services": None,   # t·∫•t c·∫£ services trong compose
            "detach": True,
            "build": False,
            "force_recreate": False,
        },
        queue=KAFKA_QUEUE,
    )

    output = wait_for_celery_result(result, timeout=600)
    print("‚úì Kafka cluster ƒë√£ ƒë∆∞·ª£c start qua Celery/RabbitMQ")
    return {
        "task_id": result.id,
        "queue": KAFKA_QUEUE,
        "compose_path": compose_path,
        "output": output,
    }


def ensure_kafka_topics_via_celery(**context):
    """
    ƒê·∫£m b·∫£o Kafka topics t·ªìn t·∫°i b·∫±ng c√°ch ch·∫°y l·ªánh tr√™n node Kafka qua Celery
    """
    cmd = (
        "docker exec kafka kafka-topics --bootstrap-server localhost:9092 "
        "--create --if-not-exists --topic house-prices-input --replication-factor 1 --partitions 1 && "
        "docker exec kafka kafka-topics --bootstrap-server localhost:9092 "
        "--create --if-not-exists --topic house-prices-output --replication-factor 1 --partitions 1 && "
        "docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list"
    )
    print(f"üöÄ G·ª≠i l·ªánh t·∫°o Kafka topics t·ªõi queue '{KAFKA_QUEUE}' qua RabbitMQ")

    result = run_command.apply_async(
        args=[cmd],
        kwargs={},
        queue=KAFKA_QUEUE,
    )

    output = wait_for_celery_result(result, timeout=300)
    print("‚úì Kafka topics ƒë√£ ƒë∆∞·ª£c ƒë·∫£m b·∫£o qua Celery/RabbitMQ")
    return {
        "task_id": result.id,
        "queue": KAFKA_QUEUE,
        "command": cmd,
        "output": output,
    }


def start_spark_via_celery(**context):
    """
    Kh·ªüi ƒë·ªông Spark master/worker tr√™n node Spark th√¥ng qua Celery/RabbitMQ
    Gi·∫£ ƒë·ªãnh c√≥ docker-compose Spark tr√™n node, v√≠ d·ª•: ~/docker-spark/docker-compose.yml
    """
    compose_path = "~/docker-spark/docker-compose.yml"
    print(f"üöÄ G·ª≠i l·ªánh docker-compose up Spark t·ªõi queue '{SPARK_QUEUE}' qua RabbitMQ")

    result = docker_compose_up.apply_async(
        args=[compose_path],
        kwargs={
            "services": ["spark-master", "spark-worker"],
            "detach": True,
            "build": False,
            "force_recreate": False,
        },
        queue=SPARK_QUEUE,
    )

    output = wait_for_celery_result(result, timeout=600)
    print("‚úì Spark cluster ƒë√£ ƒë∆∞·ª£c start qua Celery/RabbitMQ")
    return {
        "task_id": result.id,
        "queue": SPARK_QUEUE,
        "compose_path": compose_path,
        "output": output,
    }


def send_prediction_start_message(**_):
    """G·ª≠i message b·∫Øt ƒë·∫ßu prediction qua RabbitMQ"""
    try:
        client = get_rabbitmq_client()
        client.connect()
        client.publish_message(
            queue_name='prediction_status',
            message={
                'status': 'started',
                'timestamp': datetime.now().isoformat(),
                'stage': 'prediction'
            }
        )
        client.close()
        print("‚úì ƒê√£ g·ª≠i message b·∫Øt ƒë·∫ßu prediction")
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ g·ª≠i message: {e}")


def send_prediction_complete_message(**_):
    """G·ª≠i message ho√†n th√†nh prediction qua RabbitMQ"""
    try:
        client = get_rabbitmq_client()
        client.connect()
        client.publish_message(
            queue_name='prediction_status',
            message={
                'status': 'completed',
                'timestamp': datetime.now().isoformat(),
                'stage': 'prediction'
            }
        )
        client.close()
        print("‚úì ƒê√£ g·ª≠i message ho√†n th√†nh prediction")
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ g·ª≠i message: {e}")


def wait_for_streaming_complete(**kwargs):
    """ƒê·ª£i streaming ho√†n th√†nh"""
    print("‚è≥ ƒê·ª£i 5 ph√∫t ƒë·ªÉ streaming x·ª≠ l√Ω d·ªØ li·ªáu...")
    time.sleep(300)  # 5 ph√∫t
    print("‚úì Ho√†n th√†nh th·ªùi gian streaming")


# ========================================
# DAG: PREDICT STREAMING
# ========================================
with DAG(
    'predict_streaming_pipeline',
    default_args=default_args,
    description='Streaming prediction pipeline v·ªõi HDFS v√† RabbitMQ (kh√¥ng d√πng SSH)',
    schedule=None,
    catchup=False,
    tags=['prediction', 'streaming', 'kafka', 'spark', 'rabbitmq'],
) as dag:

    # Task 1: Ki·ªÉm tra c√°c service s·∫µn s√†ng
    check_rabbitmq = PythonOperator(
        task_id='check_rabbitmq',
        python_callable=check_rabbitmq_ready,
    )

    check_kafka = PythonOperator(
        task_id='check_kafka',
        python_callable=check_kafka_ready,
    )

    check_spark = PythonOperator(
        task_id='check_spark',
        python_callable=check_spark_ready,
    )

    # Task 2: Kh·ªüi ƒë·ªông Kafka cluster qua Celery/RabbitMQ
    start_kafka_remote = PythonOperator(
        task_id='start_kafka_via_celery',
        python_callable=start_kafka_via_celery,
    )

    # Task 3: ƒê·∫£m b·∫£o Kafka topics t·ªìn t·∫°i (qua Celery/RabbitMQ)
    ensure_kafka_topics = PythonOperator(
        task_id='ensure_kafka_topics_via_celery',
        python_callable=ensure_kafka_topics_via_celery,
    )

    # Task 4: Kh·ªüi ƒë·ªông Spark cluster qua Celery/RabbitMQ
    start_spark_remote = PythonOperator(
        task_id='start_spark_via_celery',
        python_callable=start_spark_via_celery,
    )

    # Task 5: G·ª≠i message b·∫Øt ƒë·∫ßu prediction
    notify_prediction_start = PythonOperator(
        task_id='notify_prediction_start',
        python_callable=send_prediction_start_message,
    )

    # Task 6: G·ª≠i d·ªØ li·ªáu streaming v√†o Kafka remote
    send_streaming_data = BashOperator(
        task_id='send_data_to_remote_kafka',
        bash_command=f"""
        cd {PROJECT_DIR}
        echo "üì§ G·ª≠i d·ªØ li·ªáu v√†o Kafka: {KAFKA_HOST}:{KAFKA_PORT}"
        python3 streaming/kafka_producer.py 1 200
        echo "‚úì ƒê√£ g·ª≠i 200 records v√†o Kafka"
        """,
    )

    # Task 7: Kh·ªüi ƒë·ªông Spark Streaming job (ch·∫°y tr√™n node Spark ho·∫∑c local t√πy c·∫•u h√¨nh Spark master)
    start_streaming_job = BashOperator(
        task_id='start_streaming_job',
        bash_command=f"""
        cd {PROJECT_DIR}
        echo "üöÄ Kh·ªüi ƒë·ªông Spark Streaming job..."
        echo "HDFS Namenode: {HDFS_NAMENODE}"
        echo "HDFS Model Dir: {HDFS_MODEL_DIR}"
        echo "Kafka Bootstrap: {KAFKA_HOST}:{KAFKA_PORT}"

        # X√≥a checkpoint c≈© ƒë·ªÉ ƒë·ªçc l·∫°i t·ª´ ƒë·∫ßu
        rm -rf /tmp/checkpoint /tmp/checkpoint-house-prices-output

        # Ch·∫°y streaming job
        spark-submit \\
            --master {SPARK_MASTER} \\
            --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 \\
            --driver-memory 4g \\
            --executor-memory 4g \\
            --num-executors 2 \\
            --executor-cores 2 \\
            --conf spark.hadoop.fs.defaultFS={HDFS_NAMENODE} \\
            --conf spark.hadoop.fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem \\
            spark_jobs/streaming_predict.py

        echo "‚úì Streaming job ƒë√£ ho√†n th√†nh"
        """,
        env={
            'HDFS_NAMENODE': HDFS_NAMENODE,
            'HDFS_MODEL_DIR': HDFS_MODEL_DIR,
            'KAFKA_BOOTSTRAP_SERVERS': f"{KAFKA_HOST}:{KAFKA_PORT}",
        },
        execution_timeout=timedelta(minutes=5)
    )

    # Task 8: ƒê·ª£i streaming x·ª≠ l√Ω
    wait_processing = PythonOperator(
        task_id='wait_for_streaming',
        python_callable=wait_for_streaming_complete,
    )

    # Task 9: G·ª≠i message ho√†n th√†nh prediction
    notify_prediction_complete = PythonOperator(
        task_id='notify_prediction_complete',
        python_callable=send_prediction_complete_message,
    )

    # Task 10: Cleanup (local)
    cleanup = BashOperator(
        task_id='cleanup',
        bash_command="""
        if [ -f /tmp/spark_streaming.pid ]; then
            PID=$(cat /tmp/spark_streaming.pid)
            echo "üõë ƒêang d·ª´ng Spark Streaming job (PID: $PID)"
            kill $PID 2>/dev/null || echo "Process ƒë√£ d·ª´ng"
            rm -rf /tmp/checkpoint /tmp/checkpoint-house-prices-output
        fi
        echo "‚úì Ho√†n th√†nh pipeline"
        """,
        trigger_rule='all_done'
    )

    # ƒê·ªãnh nghƒ©a dependencies
    [check_rabbitmq, check_kafka, check_spark] >> start_kafka_remote >> ensure_kafka_topics
    [check_spark] >> start_spark_remote
    [ensure_kafka_topics, start_spark_remote] >> notify_prediction_start >> send_streaming_data >> start_streaming_job >> wait_processing >> notify_prediction_complete >> cleanup

