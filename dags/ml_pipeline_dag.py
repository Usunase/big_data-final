"""
Airflow DAG cho há»‡ thá»‘ng phÃ¢n tÃ¡n (FIXED VERSION)
- Machine 1 (192.168.80.148): Airflow orchestrator
- Machine 2 (192.168.80.127): Kafka cluster
- Machine 3 (192.168.80.207): Spark cluster
"""
from airflow import DAG
try:
    from airflow.providers.standard.operators.bash import BashOperator
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    from airflow.operators.bash import BashOperator
    from airflow.operators.python import PythonOperator

from datetime import datetime, timedelta
import time, socket

# ========================================
# Cáº¤U HÃŒNH Há»† THá»NG PHÃ‚N TÃN
# ========================================
KAFKA_HOST, KAFKA_PORT, KAFKA_USER = "192.168.80.127", 9092, "nindang"
SPARK_HOST, SPARK_USER = "192.168.80.207", "nindang"
SPARK_MASTER = f"spark://{SPARK_HOST}:7077"
PROJECT_DIR = "/home/haminhchien/Documents/bigdata/final_project"

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# HÃ m kiá»ƒm tra sáºµn sÃ ng cho má»™t service báº¥t ká»³
def check_remote_ready(host, port, name, max_retries=10, delay=5):
    print(f"ðŸ” Kiá»ƒm tra {name} táº¡i {host}:{port}")
    for i in range(max_retries):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                if sock.connect_ex((host, port)) == 0:
                    print(f"âœ“ {name} Ä‘Ã£ sáºµn sÃ ng táº¡i {host}:{port}!")
                    return True
            print(f"â³ Chá» {name}... (láº§n {i+1}/{max_retries})")
        except Exception as e:
            print(f"âŒ Lá»—i khi kiá»ƒm tra {name}: {e}")
        time.sleep(delay)
    raise Exception(f"{name} khÃ´ng sáºµn sÃ ng táº¡i {host}:{port}")

def check_kafka_ready(**_):
    return check_remote_ready(KAFKA_HOST, KAFKA_PORT, "Kafka", max_retries=30, delay=10)

def check_spark_ready(**_):
    return check_remote_ready(SPARK_HOST, 7077, "Spark Master")
def wait_for_streaming_complete(**kwargs):
    """Äá»£i streaming hoÃ n thÃ nh (hoáº·c timeout)"""
    print("â³ Äá»£i 5 phÃºt Ä‘á»ƒ streaming xá»­ lÃ½ dá»¯ liá»‡u...")
    time.sleep(300)  # 5 phÃºt
    print("âœ“ HoÃ n thÃ nh thá»i gian streaming")
# ========================================
# DAG CHÃNH
# ========================================
with DAG(
    'ml_streaming_pipeline_distributed',
    default_args=default_args,
    description='Distributed ML pipeline: Airflow -> Kafka (remote) -> Spark (remote)',
    schedule=None,
    catchup=False,
    tags=['distributed', 'machine-learning', 'kafka', 'spark'],
) as dag:
    
    # Task 1: Khá»Ÿi Ä‘á»™ng Kafka trÃªn mÃ¡y remote
    start_kafka_remote = BashOperator(
        task_id='start_kafka_remote',
        bash_command=f"""
        echo "=========================================="
        echo "KHá»žI Äá»˜NG KAFKA CLUSTER - {KAFKA_HOST}"
        echo "=========================================="
        ssh {KAFKA_USER}@{KAFKA_HOST} '
        cd ~/Documents/bigdata/final_project/docker && \
        (docker compose down || docker-compose down || true) && \
        (docker compose up -d || docker-compose up -d) && \
        sleep 10 && docker ps | grep -E "kafka|zookeeper"
        '
        echo "âœ“ Kafka remote Ä‘Ã£ khá»Ÿi Ä‘á»™ng"
        """
    )
    
    # Task 2: Kiá»ƒm tra Kafka Ä‘Ã£ sáºµn sÃ ng
    check_kafka = PythonOperator(
        task_id='check_kafka_remote',
        python_callable=check_kafka_ready,
    )
    # Task 2b: Äáº£m báº£o Kafka topic output tá»“n táº¡i
    ensure_kafka_output_topic = BashOperator(
        task_id='ensure_kafka_output_topic',
        bash_command=f"""
        echo "=========================================="
        echo "Äáº¢M Báº¢O TOPIC house-prices-output Tá»’N Táº I"
        echo "=========================================="
        ssh {KAFKA_USER}@{KAFKA_HOST} '
        set -e
        docker exec kafka kafka-topics --bootstrap-server localhost:9092 \\
            --create --if-not-exists \\
            --topic house-prices-output \\
            --replication-factor 1 \\
            --partitions 1
        echo ""
        docker exec kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic house-prices-output
        '
        echo "âœ“ Kafka topic house-prices-output Ä‘Ã£ sáºµn sÃ ng"
        """
    )
    # Task 3: Khá»Ÿi Ä‘á»™ng Spark cluster trÃªn mÃ¡y remote
    start_spark_remote = BashOperator(
        task_id='start_spark_remote',
        bash_command=f"""
        echo "=========================================="
        echo "KHá»žI Äá»˜NG SPARK CLUSTER - {SPARK_HOST}"
        echo "=========================================="
        ssh {SPARK_USER}@{SPARK_HOST} '
        export SPARK_HOME=/home/nindang/spark-4.0.0-bin-hadoop3
        [ ! -d "$SPARK_HOME" ] && echo "âŒ SPARK_HOME not found" && exit 1
        $SPARK_HOME/sbin/stop-worker.sh 2>/dev/null || true
        $SPARK_HOME/sbin/stop-master.sh 2>/dev/null || true
        sleep 5
        $SPARK_HOME/sbin/start-master.sh && sleep 5
        $SPARK_HOME/sbin/start-worker.sh {SPARK_MASTER} && sleep 5
        jps | grep -E "Master|Worker"
        '
        echo "âœ“ Spark remote Ä‘Ã£ khá»Ÿi Ä‘á»™ng"
        """
    )
    
    # Task 4: Kiá»ƒm tra Spark Ä‘Ã£ sáºµn sÃ ng
    check_spark = PythonOperator(
        task_id='check_spark_remote',
        python_callable=check_spark_ready,
    )
    
    # Task 5: Chuáº©n bá»‹ dá»¯ liá»‡u (local - trÃªn mÃ¡y Airflow)
    prepare_data = BashOperator(
        task_id='prepare_data',
        bash_command="""
        cd {{ params.project_dir }}
        if [ -f data/train_data.csv ]; then
            echo "âœ“ Dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn"
        else
            echo "ðŸ“Š Äang chuáº©n bá»‹ dá»¯ liá»‡u..."
            python data/prepare_data.py
        fi
        """,
        params={'project_dir': PROJECT_DIR}
    )
    
    # Task 6: Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn Spark cluster remote
    train_model = BashOperator(
        task_id='train_model',
        bash_command="""
        cd {{ params.project_dir }}
        echo "ðŸš€ Gá»­i training job Ä‘áº¿n Spark: {{ params.spark_master }}"
        spark-submit \
            --master {{ params.spark_master }} \
            --conf spark.hadoop.fs.defaultFS=file:/// \
            --conf spark.local.dir=/tmp/spark_local \
            --driver-memory 4g \
            --executor-memory 4g \
            --num-executors 2 \
            --executor-cores 2 \
            spark_jobs/train_model.py
        echo "âœ“ Training hoÃ n thÃ nh"
        """,
        params={'project_dir': PROJECT_DIR, 'spark_master': SPARK_MASTER}
    )
    
    # Task 7: Gá»­i dá»¯ liá»‡u streaming vÃ o Kafka remote
    send_streaming_data = BashOperator(
        task_id='send_data_to_remote_kafka',
        bash_command=f"""
        cd {PROJECT_DIR}
        echo "ðŸ“¤ Gá»­i dá»¯ liá»‡u vÃ o Kafka: {KAFKA_HOST}:{KAFKA_PORT}"
        python streaming/kafka_producer.py 1 200
        echo "âœ“ ÄÃ£ gá»­i 200 records vÃ o Kafka"
        """
    )
    
    # Task 8: Khá»Ÿi Ä‘á»™ng Spark Streaming job (tá»± Ä‘á»™ng dá»«ng sau 2 phÃºt)
    # LÆ¯U Ã: streaming_predict.py pháº£i cÃ³ timeout logic (xem artifact streaming_predict_fixed)
    start_streaming_job = BashOperator(
        task_id='start_streaming_job',
        bash_command="""
        cd {{ params.project_dir }}
        echo "ðŸš€ Khá»Ÿi Ä‘á»™ng Spark Streaming job..."

        # XÃ³a checkpoint cÅ© Ä‘á»ƒ Ä‘á»c láº¡i tá»« Ä‘áº§u (trÃ¡nh giá»¯ offset cÅ© lÃ m trá»‘ng output)
        rm -rf /tmp/checkpoint /tmp/checkpoint-house-prices-output
        
        # Cháº¡y streaming job (foreground - Ä‘á»£i nÃ³ tá»± dá»«ng)
        spark-submit \
            --master {{ params.spark_master }} \
            --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 \
            --driver-memory 4g \
            --executor-memory 4g \
            --num-executors 2 \
            --executor-cores 2 \
            spark_jobs/streaming_predict.py
        
        echo "âœ“ Streaming job Ä‘Ã£ hoÃ n thÃ nh"
        """,
        params={
            'project_dir': PROJECT_DIR,
            'spark_master': SPARK_MASTER,
        },
        execution_timeout=timedelta(minutes=5)  # Timeout sau 5 phÃºt
    )
    wait_processing = PythonOperator(
        task_id='wait_for_streaming',
        python_callable=wait_for_streaming_complete,
    )
    cleanup = BashOperator(
        task_id='cleanup',
        bash_command="""
        if [ -f /tmp/spark_streaming.pid ]; then
            PID=$(cat /tmp/spark_streaming.pid)
            echo "ðŸ›‘ Äang dá»«ng Spark Streaming job (PID: $PID)"
            kill $PID 2>/dev/null || echo "Process Ä‘Ã£ dá»«ng"
            rm -rf /tmp/checkpoint /tmp/checkpoint-house-prices-output
        fi
        echo "âœ“ HoÃ n thÃ nh pipeline"
        """,
        trigger_rule='all_done'  # Cháº¡y dÃ¹ task trÆ°á»›c thÃ nh cÃ´ng hay tháº¥t báº¡i
    )
    
    # Äá»‹nh nghÄ©a dependencies
    start_kafka_remote >> check_kafka >> ensure_kafka_output_topic
    start_spark_remote >> check_spark
    [ensure_kafka_output_topic, check_spark] >> prepare_data >> train_model >> send_streaming_data >> start_streaming_job >> wait_processing >> cleanup


# ========================================
# DAG VISUALIZATION
# ========================================
with DAG(
    'ml_streaming_visualization',
    default_args=default_args,
    description='Run visualization consumer',
    schedule=None,  # Cháº¡y manual
    catchup=False,
    tags=['visualization', 'kafka'],
) as dag_viz:

    run_visualization = BashOperator(
        task_id='run_visualization',
        bash_command="""
        cd {{ params.project_dir }} && \
        python visualization/kafka_consumer.py
        """,
        params={'project_dir': PROJECT_DIR}
    )