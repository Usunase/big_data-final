"""
Airflow DAG Ä‘á»ƒ Ä‘iá»u khiá»ƒn toÃ n bá»™ ML pipeline
TÆ°Æ¡ng thÃ­ch vá»›i Airflow 3.1.3
ÄÃƒ Sá»¬A: Gá»­i dá»¯ liá»‡u vÃ o Kafka TRÆ¯á»šC KHI khá»Ÿi Ä‘á»™ng Spark Streaming
"""
from airflow import DAG
# Airflow 3.x: Sá»­ dá»¥ng providers.standard thay vÃ¬ operators cÅ©
try:
    from airflow.providers.standard.operators.bash import BashOperator
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    # Fallback cho Airflow 2.x
    from airflow.operators.bash import BashOperator
    from airflow.operators.python import PythonOperator

from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime, timedelta
import time
import subprocess
import os

# Default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def check_kafka_ready(**kwargs):
    """Kiá»ƒm tra Kafka Ä‘Ã£ sáºµn sÃ ng chÆ°a"""
    import socket
    max_retries = 30
    for i in range(max_retries):
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(('localhost', 9092))
            sock.close()
            if result == 0:
                print(f"âœ“ Kafka Ä‘Ã£ sáºµn sÃ ng!")
                return True
            else:
                print(f"â³ Äang chá» Kafka... (thá»­ láº§n {i+1}/{max_retries})")
                time.sleep(10)
        except Exception as e:
            print(f"âŒ Lá»—i khi kiá»ƒm tra Kafka: {e}")
            time.sleep(10)
    raise Exception("Kafka khÃ´ng sáºµn sÃ ng sau 5 phÃºt")

def wait_for_streaming_complete(**kwargs):
    """Äá»£i streaming hoÃ n thÃ nh (hoáº·c timeout)"""
    print("â³ Äá»£i 5 phÃºt Ä‘á»ƒ streaming xá»­ lÃ½ dá»¯ liá»‡u...")
    time.sleep(300)  # 5 phÃºt
    print("âœ“ HoÃ n thÃ nh thá»i gian streaming")

# Táº¡o DAG
with DAG(
    'ml_streaming_pipeline',
    default_args=default_args,
    description='End-to-end ML pipeline with Kafka and Spark',
    schedule=None,  # Cháº¡y manual (schedule_interval deprecated in Airflow 2.4+)
    catchup=False,
    tags=['machine-learning', 'kafka', 'spark', 'streaming'],
) as dag:
    
    # Task 1: Khá»Ÿi Ä‘á»™ng Kafka vá»›i Docker Compose
    start_kafka = BashOperator(
        task_id='start_kafka',
        bash_command="""
        set -e
        export PATH=/usr/bin:$PATH
        cd {{ params.project_dir }}/docker
        echo "Current directory: $(pwd)"
        echo "Docker version: $(docker --version)"
        echo "Docker compose version: $(docker compose version || echo 'docker compose not found, trying docker-compose')"
        docker compose down || docker-compose down || true
        docker compose up -d || docker-compose up -d
        sleep 5
        docker ps | grep -E "kafka|zookeeper" || echo "Warning: Containers may not be running"
        echo "âœ“ ÄÃ£ khá»Ÿi Ä‘á»™ng Kafka container"
        """,
        params={'project_dir': '/home/haminhchien/Documents/bigdata/final_project'}
    )
    
    # Task 2: Kiá»ƒm tra Kafka Ä‘Ã£ sáºµn sÃ ng
    check_kafka = PythonOperator(
        task_id='check_kafka_ready',
        python_callable=check_kafka_ready,
    )
    
    # Task 3: Chuáº©n bá»‹ dá»¯ liá»‡u (náº¿u chÆ°a cÃ³)
    prepare_data = BashOperator(
        task_id='prepare_data',
        bash_command="""
        cd {{ params.project_dir }} && \
        if [ ! -f data/train_data.csv ]; then
            echo "ðŸ“Š Äang chuáº©n bá»‹ dá»¯ liá»‡u..."
            python data/prepare_data.py
        else
            echo "âœ“ Dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn"
        fi
        """,
        params={'project_dir': '/home/haminhchien/Documents/bigdata/final_project'}
    )
    
    # Task 4: Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i Spark (dÃ¹ng bash Ä‘á»ƒ kiá»ƒm soÃ¡t --master)
    train_model = BashOperator(
        task_id='train_model',
        bash_command="""
        cd {{ params.project_dir }} && \
        spark-submit \
            --master spark://192.168.80.207:7077 \
            --conf spark.hadoop.fs.defaultFS=file:/// \
            --conf spark.local.dir=/tmp/spark_local \
            --driver-memory 4g \
            --executor-memory 4g \
            --num-executors 2 \
            --executor-cores 2 \
            spark_jobs/train_model.py
        """,
        params={'project_dir': '/home/haminhchien/Documents/bigdata/final_project'}
    )
    
    # Task 5: Gá»­i dá»¯ liá»‡u streaming vÃ o Kafka (ÄÃƒ Sá»¬A: Chuyá»ƒn lÃªn trÆ°á»›c)
    send_streaming_data = BashOperator(
        task_id='send_streaming_data',
        bash_command="""
        cd {{ params.project_dir }} && \
        echo "ðŸ“¤ Äang gá»­i dá»¯ liá»‡u streaming vÃ o Kafka..." && \
        python streaming/kafka_producer.py 1 200
        """,
        params={'project_dir': '/home/haminhchien/Documents/bigdata/final_project'}
    )
    
    # Task 6: Khá»Ÿi Ä‘á»™ng Spark Streaming job (ÄÃƒ Sá»¬A: Chuyá»ƒn xuá»‘ng sau)
    start_streaming_job = BashOperator(
        task_id='start_streaming_job',
        bash_command="""
        cd {{ params.project_dir }} && \
        nohup spark-submit \
            --master spark://192.168.80.207:7077 \
            --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 \
            --driver-memory 4g \
            --executor-memory 4g \
            --num-executors 2 \
            --executor-cores 2 \
            spark_jobs/streaming_predict.py > /tmp/spark_streaming.log 2>&1 &
        echo $! > /tmp/spark_streaming.pid
        echo "âœ“ ÄÃ£ khá»Ÿi Ä‘á»™ng Spark Streaming job (PID: $(cat /tmp/spark_streaming.pid))"
        sleep 20
        """,
        params={'project_dir': '/home/haminhchien/Documents/bigdata/final_project'}
    )
    
    # Task 7: Äá»£i streaming xá»­ lÃ½ xong
    wait_processing = PythonOperator(
        task_id='wait_for_streaming',
        python_callable=wait_for_streaming_complete,
    )
    
    # Task 8: Dá»n dáº¹p (optional - dá»«ng streaming job)
    cleanup = BashOperator(
        task_id='cleanup',
        bash_command="""
        if [ -f /tmp/spark_streaming.pid ]; then
            PID=$(cat /tmp/spark_streaming.pid)
            echo "ðŸ›‘ Äang dá»«ng Spark Streaming job (PID: $PID)"
            kill $PID 2>/dev/null || echo "Process Ä‘Ã£ dá»«ng"
            rm /tmp/spark_streaming.pid
        fi
        echo "âœ“ HoÃ n thÃ nh pipeline"
        """,
        trigger_rule='all_done'  # Cháº¡y dÃ¹ task trÆ°á»›c thÃ nh cÃ´ng hay tháº¥t báº¡i
    )
    
    # ÄÃƒ Sá»¬A: Äá»‹nh nghÄ©a thá»© tá»± thá»±c thi má»›i
    # Dá»¯ liá»‡u Ä‘Æ°á»£c gá»­i vÃ o Kafka TRÆ¯á»šC, sau Ä‘Ã³ má»›i khá»Ÿi Ä‘á»™ng Streaming job Ä‘á»ƒ xá»­ lÃ½
    start_kafka >> check_kafka >> prepare_data >> train_model >> send_streaming_data >> start_streaming_job >> wait_processing >> cleanup


# DAG riÃªng Ä‘á»ƒ cháº¡y visualization
with DAG(
    'ml_streaming_visualization',
    default_args=default_args,
    description='Run visualization consumer',
    schedule=None,  # Cháº¡y manual
    catchup=False,
    tags=['visualization', 'kafka'],
) as dag_viz:
    
    run_visualization = BashOperator(
        task_id='run_visualization',
        bash_command="""
        cd {{ params.project_dir }} && \
        python visualization/kafka_consumer.py
        """,
        params={'project_dir': '/home/haminhchien/Documents/bigdata/final_project'}
    )