"""
Airflow DAG cho h·ªá th·ªëng ph√¢n t√°n (kh√¥ng d√πng SSH)
- Machine 1 (Airflow + RabbitMQ): Orchestrator
- Machine 2 (192.168.80.127): Kafka cluster (Celery queue: node_57)
- Machine 3 (192.168.80.207): Spark cluster (Celery queue: spark)
"""
from airflow import DAG
try:
    from airflow.providers.standard.operators.bash import BashOperator
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    from airflow.operators.bash import BashOperator
    from airflow.operators.python import PythonOperator

from datetime import datetime, timedelta
import time
import socket

from mycelery.system_worker import docker_compose_up, run_command

# ========================================
# C·∫§U H√åNH H·ªÜ TH·ªêNG PH√ÇN T√ÅN
# ========================================
KAFKA_HOST, KAFKA_PORT = "192.168.80.127", 9092
SPARK_HOST = "192.168.80.207"
SPARK_MASTER = f"spark://{SPARK_HOST}:7077"
PROJECT_DIR = "/home/haminhchien/Documents/bigdata/final_project"

# Queue mapping (kh·ªõp v·ªõi CLUSTER_NODES trong system_worker.py)
KAFKA_QUEUE = "node_57"
SPARK_QUEUE = "spark"

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


def check_remote_ready(host, port, name, max_retries=10, delay=5):
    """Ki·ªÉm tra service TCP ƒë√£ s·∫µn s√†ng (Kafka/Spark)"""
    print(f"üîç Ki·ªÉm tra {name} t·∫°i {host}:{port}")
    for i in range(max_retries):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                if sock.connect_ex((host, port)) == 0:
                    print(f"‚úì {name} ƒë√£ s·∫µn s√†ng t·∫°i {host}:{port}!")
                    return True
            print(f"‚è≥ Ch·ªù {name}... (l·∫ßn {i+1}/{max_retries})")
        except Exception as e:
            print(f"‚ùå L·ªói khi ki·ªÉm tra {name}: {e}")
        time.sleep(delay)
    raise Exception(f"{name} kh√¥ng s·∫µn s√†ng t·∫°i {host}:{port}")


def check_kafka_ready(**_):
    return check_remote_ready(KAFKA_HOST, KAFKA_PORT, "Kafka", max_retries=30, delay=10)


def check_spark_ready(**_):
    return check_remote_ready(SPARK_HOST, 7077, "Spark Master")


def wait_for_streaming_complete(**kwargs):
    """ƒê·ª£i streaming ho√†n th√†nh (ho·∫∑c timeout)"""
    print("‚è≥ ƒê·ª£i 5 ph√∫t ƒë·ªÉ streaming x·ª≠ l√Ω d·ªØ li·ªáu...")
    time.sleep(300)  # 5 ph√∫t
    print("‚úì Ho√†n th√†nh th·ªùi gian streaming")


def wait_for_celery_result(result, timeout=600, poll_interval=5):
    """ƒê·ª£i Celery task ho√†n th√†nh qua RabbitMQ"""
    elapsed = 0
    while elapsed < timeout:
        if result.ready():
            if result.successful():
                return result.result
            raise Exception(f"Celery task failed: {result.result}")
        time.sleep(poll_interval)
        elapsed += poll_interval
    raise TimeoutError(f"Celery task {result.id} timed out sau {timeout} gi√¢y")


def start_kafka_via_celery(**context):
    """
    Kh·ªüi ƒë·ªông Kafka cluster tr√™n node Kafka th√¥ng qua Celery/RabbitMQ
    Gi·∫£ ƒë·ªãnh c√≥ docker-compose kafka tr√™n node, v√≠ d·ª•: ~/kafka-cluster/docker-compose.yml
    """
    compose_path = "~/kafka-cluster/docker-compose.yml"
    print(f"üöÄ G·ª≠i l·ªánh docker-compose up Kafka t·ªõi queue '{KAFKA_QUEUE}'")

    result = docker_compose_up.apply_async(
        args=[compose_path],
        kwargs={
            "services": None,
            "detach": True,
            "build": False,
            "force_recreate": False,
        },
        queue=KAFKA_QUEUE,
    )

    output = wait_for_celery_result(result, timeout=600)
    print("‚úì Kafka cluster ƒë√£ ƒë∆∞·ª£c start qua Celery/RabbitMQ")
    return {
        "task_id": result.id,
        "queue": KAFKA_QUEUE,
        "compose_path": compose_path,
        "output": output,
    }


def ensure_kafka_output_topic_via_celery(**context):
    """
    ƒê·∫£m b·∫£o Kafka topic house-prices-output t·ªìn t·∫°i b·∫±ng c√°ch ch·∫°y l·ªánh tr√™n node Kafka
    """
    cmd = (
        "docker exec kafka kafka-topics --bootstrap-server localhost:9092 "
        "--create --if-not-exists --topic house-prices-output --replication-factor 1 --partitions 1 && "
        "docker exec kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic house-prices-output"
    )
    print(f"üöÄ G·ª≠i l·ªánh t·∫°o Kafka topic output t·ªõi queue '{KAFKA_QUEUE}'")

    result = run_command.apply_async(
        args=[cmd],
        kwargs={},
        queue=KAFKA_QUEUE,
    )

    output = wait_for_celery_result(result, timeout=300)
    print("‚úì Kafka topic house-prices-output ƒë√£ ƒë∆∞·ª£c ƒë·∫£m b·∫£o qua Celery/RabbitMQ")
    return {
        "task_id": result.id,
        "queue": KAFKA_QUEUE,
        "command": cmd,
        "output": output,
    }


def start_spark_via_celery(**context):
    """
    Kh·ªüi ƒë·ªông Spark master/worker tr√™n node Spark th√¥ng qua Celery/RabbitMQ
    Gi·∫£ ƒë·ªãnh c√≥ docker-compose Spark tr√™n node, v√≠ d·ª•: ~/docker-spark/docker-compose.yml
    """
    compose_path = "~/docker-spark/docker-compose.yml"
    print(f"üöÄ G·ª≠i l·ªánh docker-compose up Spark t·ªõi queue '{SPARK_QUEUE}'")

    result = docker_compose_up.apply_async(
        args=[compose_path],
        kwargs={
            "services": ["spark-master", "spark-worker"],
            "detach": True,
            "build": False,
            "force_recreate": False,
        },
        queue=SPARK_QUEUE,
    )

    output = wait_for_celery_result(result, timeout=600)
    print("‚úì Spark cluster ƒë√£ ƒë∆∞·ª£c start qua Celery/RabbitMQ")
    return {
        "task_id": result.id,
        "queue": SPARK_QUEUE,
        "compose_path": compose_path,
        "output": output,
    }


# ========================================
# DAG CH√çNH
# ========================================
with DAG(
    'ml_streaming_pipeline_distributed',
    default_args=default_args,
    description='Distributed ML pipeline: Airflow -> Kafka/Spark qua RabbitMQ (kh√¥ng d√πng SSH)',
    schedule=None,
    catchup=False,
    tags=['distributed', 'machine-learning', 'kafka', 'spark'],
) as dag:

    # Task 1: Kh·ªüi ƒë·ªông Kafka tr√™n m√°y remote qua Celery/RabbitMQ
    start_kafka_remote = PythonOperator(
        task_id='start_kafka_via_celery',
        python_callable=start_kafka_via_celery,
    )

    # Task 2: Ki·ªÉm tra Kafka ƒë√£ s·∫µn s√†ng
    check_kafka = PythonOperator(
        task_id='check_kafka_remote',
        python_callable=check_kafka_ready,
    )

    # Task 2b: ƒê·∫£m b·∫£o Kafka topic output t·ªìn t·∫°i qua Celery/RabbitMQ
    ensure_kafka_output_topic = PythonOperator(
        task_id='ensure_kafka_output_topic_via_celery',
        python_callable=ensure_kafka_output_topic_via_celery,
    )

    # Task 3: Kh·ªüi ƒë·ªông Spark cluster tr√™n m√°y remote qua Celery/RabbitMQ
    start_spark_remote = PythonOperator(
        task_id='start_spark_via_celery',
        python_callable=start_spark_via_celery,
    )

    # Task 4: Ki·ªÉm tra Spark ƒë√£ s·∫µn s√†ng
    check_spark = PythonOperator(
        task_id='check_spark_remote',
        python_callable=check_spark_ready,
    )

    # Task 5: Chu·∫©n b·ªã d·ªØ li·ªáu (local - tr√™n m√°y Airflow)
    prepare_data = BashOperator(
        task_id='prepare_data',
        bash_command="""
        cd {{ params.project_dir }}
        if [ -f data/train_data.csv ]; then
            echo "‚úì D·ªØ li·ªáu ƒë√£ c√≥ s·∫µn"
        else
            echo "üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu..."
            python data/prepare_data.py
        fi
        """,
        params={'project_dir': PROJECT_DIR}
    )

    # Task 6: Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n Spark cluster (Spark ƒë√£ ƒë∆∞·ª£c start s·∫µn)
    train_model = BashOperator(
        task_id='train_model',
        bash_command="""
        cd {{ params.project_dir }}
        echo "üöÄ G·ª≠i training job ƒë·∫øn Spark: {{ params.spark_master }}"
        spark-submit \
            --master {{ params.spark_master }} \
            --conf spark.hadoop.fs.defaultFS=file:/// \
            --conf spark.local.dir=/tmp/spark_local \
            --driver-memory 4g \
            --executor-memory 4g \
            --num-executors 2 \
            --executor-cores 2 \
            spark_jobs/train_model.py
        echo "‚úì Training ho√†n th√†nh"
        """,
        params={'project_dir': PROJECT_DIR, 'spark_master': SPARK_MASTER}
    )

    # Task 7: G·ª≠i d·ªØ li·ªáu streaming v√†o Kafka remote
    send_streaming_data = BashOperator(
        task_id='send_data_to_remote_kafka',
        bash_command=f"""
        cd {PROJECT_DIR}
        echo "üì§ G·ª≠i d·ªØ li·ªáu v√†o Kafka: {KAFKA_HOST}:{KAFKA_PORT}"
        python3 streaming/kafka_producer.py 1 200
        echo "‚úì ƒê√£ g·ª≠i 200 records v√†o Kafka"
        """
    )

    # Task 8: Kh·ªüi ƒë·ªông Spark Streaming job (t·ª± ƒë·ªông d·ª´ng sau 2 ph√∫t)
    start_streaming_job = BashOperator(
        task_id='start_streaming_job',
        bash_command="""
        cd {{ params.project_dir }}
        echo "üöÄ Kh·ªüi ƒë·ªông Spark Streaming job..."

        # X√≥a checkpoint c≈© ƒë·ªÉ ƒë·ªçc l·∫°i t·ª´ ƒë·∫ßu (tr√°nh gi·ªØ offset c≈© l√†m tr·ªëng output)
        rm -rf /tmp/checkpoint /tmp/checkpoint-house-prices-output

        # Ch·∫°y streaming job (foreground - ƒë·ª£i n√≥ t·ª± d·ª´ng)
        spark-submit \
            --master {{ params.spark_master }} \
            --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 \
            --driver-memory 4g \
            --executor-memory 4g \
            --num-executors 2 \
            --executor-cores 2 \
            spark_jobs/streaming_predict.py

        echo "‚úì Streaming job ƒë√£ ho√†n th√†nh"
        """,
        params={
            'project_dir': PROJECT_DIR,
            'spark_master': SPARK_MASTER,
        },
        execution_timeout=timedelta(minutes=5)  # Timeout sau 5 ph√∫t
    )

    wait_processing = PythonOperator(
        task_id='wait_for_streaming',
        python_callable=wait_for_streaming_complete,
    )

    cleanup = BashOperator(
        task_id='cleanup',
        bash_command="""
        if [ -f /tmp/spark_streaming.pid ]; then
            PID=$(cat /tmp/spark_streaming.pid)
            echo "üõë ƒêang d·ª´ng Spark Streaming job (PID: $PID)"
            kill $PID 2>/dev/null || echo "Process ƒë√£ d·ª´ng"
            rm -rf /tmp/checkpoint /tmp/checkpoint-house-prices-output
        fi
        echo "‚úì Ho√†n th√†nh pipeline"
        """,
        trigger_rule='all_done'  # Ch·∫°y d√π task tr∆∞·ªõc th√†nh c√¥ng hay th·∫•t b·∫°i
    )

    # ƒê·ªãnh nghƒ©a dependencies
    start_kafka_remote >> check_kafka >> ensure_kafka_output_topic
    start_spark_remote >> check_spark
    [ensure_kafka_output_topic, check_spark] >> prepare_data >> train_model >> send_streaming_data >> start_streaming_job >> wait_processing >> cleanup


# ========================================
# DAG VISUALIZATION
# ========================================
with DAG(
    'ml_streaming_visualization',
    default_args=default_args,
    description='Run visualization consumer',
    schedule=None,  # Ch·∫°y manual
    catchup=False,
    tags=['visualization', 'kafka'],
) as dag_viz:

    run_visualization = BashOperator(
        task_id='run_visualization',
        bash_command="""
        cd {{ params.project_dir }} && \
        python visualization/kafka_consumer.py
        """,
        params={'project_dir': PROJECT_DIR}
    )