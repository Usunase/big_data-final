"""
Spark Structured Streaming job - Äá»c tá»« Kafka, dá»± Ä‘oÃ¡n vÃ  gá»­i láº¡i káº¿t quáº£
ÄÃƒ Sá»¬A: ThÃªm timeout Ä‘á»ƒ tá»± Ä‘á»™ng dá»«ng sau khi xá»­ lÃ½ xong
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, to_json, struct
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
from pyspark.ml import PipelineModel
import time
import sys

def streaming_prediction():
    # Khá»Ÿi táº¡o Spark Session
    spark = SparkSession.builder \
        .appName("HousePriceStreamingPrediction") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0") \
        .config("spark.hadoop.fs.defaultFS", "file:///") \
        .config("spark.hadoop.fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem") \
        .getOrCreate()

    
    spark.sparkContext.setLogLevel("WARN")
    
    print("=" * 60)
    print("SPARK STREAMING - Dá»° ÄOÃN GIÃ NHÃ€")
    print("=" * 60)
    
    # Load mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
    model_path = "models/house_price_model"
    print(f"ğŸ“‚ Äang táº£i mÃ´ hÃ¬nh tá»«: {model_path}")
    try:
        model = PipelineModel.load(model_path)
        print("âœ“ ÄÃ£ táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng")
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh: {e}")
        spark.stop()
        sys.exit(1)
    
    # Schema cho dá»¯ liá»‡u tá»« Kafka
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("MedInc", DoubleType(), True),
        StructField("HouseAge", DoubleType(), True),
        StructField("AveRooms", DoubleType(), True),
        StructField("AveBedrms", DoubleType(), True),
        StructField("Population", DoubleType(), True),
        StructField("AveOccup", DoubleType(), True),
        StructField("Latitude", DoubleType(), True),
        StructField("Longitude", DoubleType(), True),
        StructField("actual_price", DoubleType(), True)
    ])
    
    # Äá»c dá»¯ liá»‡u tá»« Kafka
    print("ğŸ“¥ Äang káº¿t ná»‘i Ä‘áº¿n Kafka topic: house-prices-input")
    df_stream = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "192.168.80.127:9092") \
        .option("subscribe", "house-prices-input") \
        .option("startingOffsets", "earliest") \
        .load()
    
    # Parse JSON
    df_parsed = df_stream.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    # Dá»± Ä‘oÃ¡n
    predictions = model.transform(df_parsed)
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ gá»­i láº¡i Kafka
    result = predictions.select(
        col("id"),
        col("actual_price"),
        col("prediction").alias("predicted_price"),
        (col("prediction") - col("actual_price")).alias("error"),
        ((col("prediction") - col("actual_price")) / col("actual_price") * 100).alias("error_percentage")
    )
    
    # Chuyá»ƒn thÃ nh JSON Ä‘á»ƒ gá»­i vÃ o Kafka
    kafka_output = result.select(
        to_json(struct("*")).alias("value")
    )
    
    # Ghi káº¿t quáº£ vÃ o Kafka topic má»›i
    query = kafka_output \
        .writeStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "192.168.80.127:9092") \
        .option("topic", "house-prices-output") \
        .option("checkpointLocation", "/tmp/checkpoint") \
        .start()
    
    # Console output Ä‘á»ƒ debug
    console_query = result \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .start()
    
    print("=" * 60)
    print("âœ“ Streaming Ä‘Ã£ báº¯t Ä‘áº§u!")
    print("ğŸ“Š Äang xá»­ lÃ½ dá»¯ liá»‡u vÃ  gá»­i káº¿t quáº£ vÃ o: house-prices-output")
    print("=" * 60)
    
    # ÄÃƒ Sá»¬A: ThÃªm timeout Ä‘á»ƒ tá»± Ä‘á»™ng dá»«ng
    timeout_seconds = 120  # 2 phÃºt
    print(f"â° Streaming sáº½ cháº¡y trong {timeout_seconds} giÃ¢y")
    
    try:
        # Äá»£i vá»›i timeout
        start_time = time.time()
        while time.time() - start_time < timeout_seconds:
            if not query.isActive or not console_query.isActive:
                print("âš ï¸  Query Ä‘Ã£ dá»«ng báº¥t ngá»")
                break
            time.sleep(5)  # Check má»—i 5 giÃ¢y
        
        print(f"\nâœ“ ÄÃ£ hoÃ n thÃ nh streaming sau {int(time.time() - start_time)} giÃ¢y")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Nháº­n Ä‘Æ°á»£c tÃ­n hiá»‡u dá»«ng...")
    
    finally:
        print("ğŸ›‘ Äang dá»«ng streaming queries...")
        query.stop()
        console_query.stop()
        spark.stop()
        print("âœ“ ÄÃ£ dá»«ng streaming hoÃ n toÃ n")

if __name__ == "__main__":
    streaming_prediction()