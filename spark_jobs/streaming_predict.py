"""
Spark Structured Streaming job - ƒê·ªçc t·ª´ Kafka, d·ª± ƒëo√°n v√† g·ª≠i l·∫°i k·∫øt qu·∫£
ƒê·ªçc model t·ª´ HDFS
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, to_json, struct
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
from pyspark.ml import PipelineModel
import time
import sys
import os

# C·∫•u h√¨nh HDFS
HDFS_NAMENODE = os.getenv("HDFS_NAMENODE", "hdfs://192.168.80.148:9000")
HDFS_MODEL_DIR = os.getenv("HDFS_MODEL_DIR", "/bigdata/house_prices/models")
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "192.168.80.127:9092")

def streaming_prediction():
    # Kh·ªüi t·∫°o Spark Session v·ªõi HDFS
    spark = SparkSession.builder \
        .appName("HousePriceStreamingPrediction") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0") \
        .config("spark.hadoop.fs.defaultFS", HDFS_NAMENODE) \
        .config("spark.hadoop.fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem") \
        .getOrCreate()

    
    spark.sparkContext.setLogLevel("WARN")
    
    print("=" * 60)
    print("SPARK STREAMING - D·ª∞ ƒêO√ÅN GI√Å NH√Ä")
    print("=" * 60)
    print(f"HDFS Namenode: {HDFS_NAMENODE}")
    print(f"HDFS Model Dir: {HDFS_MODEL_DIR}")
    
    # Load m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán t·ª´ HDFS
    hdfs_model_path = f"{HDFS_MODEL_DIR}/house_price_model"
    print(f"üìÇ ƒêang t·∫£i m√¥ h√¨nh t·ª´ HDFS: {hdfs_model_path}")
    try:
        model = PipelineModel.load(hdfs_model_path)
        print("‚úì ƒê√£ t·∫£i m√¥ h√¨nh th√†nh c√¥ng t·ª´ HDFS")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh t·ª´ HDFS: {e}")
        # Fallback: th·ª≠ load t·ª´ local
        local_model_path = "models/house_price_model"
        print(f"‚ö†Ô∏è  Th·ª≠ t·∫£i t·ª´ local: {local_model_path}")
        try:
            model = PipelineModel.load(local_model_path)
            print("‚úì ƒê√£ t·∫£i m√¥ h√¨nh t·ª´ local (fallback)")
        except Exception as e2:
            print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh t·ª´ local: {e2}")
            spark.stop()
            sys.exit(1)
    
    # Schema cho d·ªØ li·ªáu t·ª´ Kafka
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("MedInc", DoubleType(), True),
        StructField("HouseAge", DoubleType(), True),
        StructField("AveRooms", DoubleType(), True),
        StructField("AveBedrms", DoubleType(), True),
        StructField("Population", DoubleType(), True),
        StructField("AveOccup", DoubleType(), True),
        StructField("Latitude", DoubleType(), True),
        StructField("Longitude", DoubleType(), True),
        StructField("actual_price", DoubleType(), True)
    ])
    
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
    print(f"üì• ƒêang k·∫øt n·ªëi ƒë·∫øn Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
    print("üì• Topic: house-prices-input")
    df_stream = (
        spark.readStream 
        .format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
        .option("subscribe", "house-prices-input")
        .option("startingOffsets", "earliest")
        .option("failOnDataLoss", "false")  # kh√¥ng fail n·∫øu offset b·ªã l√πi/reset
        .load()
    )
    
    # Parse JSON
    df_parsed = df_stream.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    # D·ª± ƒëo√°n
    predictions = model.transform(df_parsed)
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ g·ª≠i l·∫°i Kafka
    result = predictions.select(
        col("id"),
        col("actual_price"),
        col("prediction").alias("predicted_price"),
        (col("prediction") - col("actual_price")).alias("error"),
        ((col("prediction") - col("actual_price")) / col("actual_price") * 100).alias("error_percentage")
    )
    
    # Chuy·ªÉn th√†nh JSON ƒë·ªÉ g·ª≠i v√†o Kafka
    kafka_output = result.select(
        to_json(struct("*")).alias("value")
    )
    
    # Ghi k·∫øt qu·∫£ v√†o Kafka topic m·ªõi
    query = (
        kafka_output.writeStream
        .format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
        .option("topic", "house-prices-output")
        .option("checkpointLocation", "/tmp/checkpoint-house-prices-output")
        .start()
    )
    
    # Console output ƒë·ªÉ debug
    console_query = result \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .start()
    
    print("=" * 60)
    print("‚úì Streaming ƒë√£ b·∫Øt ƒë·∫ßu!")
    print("üìä ƒêang x·ª≠ l√Ω d·ªØ li·ªáu v√† g·ª≠i k·∫øt qu·∫£ v√†o: house-prices-output")
    print("=" * 60)
    
    # ƒê√É S·ª¨A: Th√™m timeout ƒë·ªÉ t·ª± ƒë·ªông d·ª´ng
    timeout_seconds = 120  # 2 ph√∫t
    print(f"‚è∞ Streaming s·∫Ω ch·∫°y trong {timeout_seconds} gi√¢y")
    
    try:
        # ƒê·ª£i v·ªõi timeout
        start_time = time.time()
        while time.time() - start_time < timeout_seconds:
            if not query.isActive or not console_query.isActive:
                print("‚ö†Ô∏è  Query ƒë√£ d·ª´ng b·∫•t ng·ªù")
                break
            time.sleep(5)  # Check m·ªói 5 gi√¢y
        
        print(f"\n‚úì ƒê√£ ho√†n th√†nh streaming sau {int(time.time() - start_time)} gi√¢y")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu d·ª´ng...")
    
    finally:
        print("üõë ƒêang d·ª´ng streaming queries...")
        query.stop()
        console_query.stop()
        spark.stop()
        print("‚úì ƒê√£ d·ª´ng streaming ho√†n to√†n")

if __name__ == "__main__":
    streaming_prediction()