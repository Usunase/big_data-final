# BÃO CÃO CHI TIáº¾T: Há»† THá»NG MACHINE LEARNING STREAMING Vá»šI SPARK, KAFKA VÃ€ AIRFLOW

## Má»¤C Lá»¤C

1. [Tá»•ng quan dá»± Ã¡n](#1-tá»•ng-quan-dá»±-Ã¡n)
2. [Kiáº¿n trÃºc há»‡ thá»‘ng](#2-kiáº¿n-trÃºc-há»‡-thá»‘ng)
3. [PhÃ¢n tÃ­ch chi tiáº¿t cÃ¡c thÃ nh pháº§n](#3-phÃ¢n-tÃ­ch-chi-tiáº¿t-cÃ¡c-thÃ nh-pháº§n)
4. [Luá»“ng xá»­ lÃ½ dá»¯ liá»‡u](#4-luá»“ng-xá»­-lÃ½-dá»¯-liá»‡u)
5. [Káº¿t quáº£ vÃ  Ä‘Ã¡nh giÃ¡](#5-káº¿t-quáº£-vÃ -Ä‘Ã¡nh-giÃ¡)
6. [Káº¿t luáº­n](#6-káº¿t-luáº­n)

---

## 1. Tá»”NG QUAN Dá»° ÃN

### 1.1. Má»¥c tiÃªu

Dá»± Ã¡n xÃ¢y dá»±ng má»™t há»‡ thá»‘ng Machine Learning streaming end-to-end Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  real-time sá»­ dá»¥ng:
- **Apache Spark ML**: Huáº¥n luyá»‡n vÃ  dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh Random Forest
- **Apache Kafka**: Há»‡ thá»‘ng message queue cho streaming data
- **Apache Airflow**: Orchestration vÃ  Ä‘iá»u phá»‘i toÃ n bá»™ pipeline
- **Python**: NgÃ´n ngá»¯ láº­p trÃ¬nh chÃ­nh cho cÃ¡c thÃ nh pháº§n

### 1.2. Kiáº¿n trÃºc phÃ¢n tÃ¡n

Há»‡ thá»‘ng Ä‘Æ°á»£c triá»ƒn khai trÃªn kiáº¿n trÃºc phÃ¢n tÃ¡n gá»“m 3 mÃ¡y:
- **Machine 1 (192.168.80.147)**: Airflow orchestrator
- **Machine 2 (192.168.80.127)**: Kafka cluster
- **Machine 3 (192.168.80.207)**: Spark cluster

### 1.3. Dataset

Sá»­ dá»¥ng **California Housing Dataset** tá»« scikit-learn vá»›i cÃ¡c Ä‘áº·c trÆ°ng:
- `MedInc`: Thu nháº­p trung bÃ¬nh
- `HouseAge`: Tuá»•i nhÃ 
- `AveRooms`: Sá»‘ phÃ²ng trung bÃ¬nh
- `AveBedrms`: Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh
- `Population`: DÃ¢n sá»‘
- `AveOccup`: Máº­t Ä‘á»™ chiáº¿m dá»¥ng trung bÃ¬nh
- `Latitude`: VÄ© Ä‘á»™
- `Longitude`: Kinh Ä‘á»™
- `target`: GiÃ¡ nhÃ  trung bÃ¬nh (Ä‘Æ¡n vá»‹: $100,000)

---

## 2. KIáº¾N TRÃšC Há»† THá»NG

### 2.1. SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow DAG   â”‚ (Orchestration)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€> Prepare Data â”€â”€> Train Model (Spark ML)
         â”‚
         â”œâ”€â”€> Start Kafka â”€â”€> Start Spark Streaming
         â”‚
         â””â”€â”€> Producer â”€â”€> Kafka â”€â”€> Spark Streaming â”€â”€> Kafka Output
                                              â”‚
                                              â””â”€â”€> Visualization
```

### 2.2. CÃ¡c thÃ nh pháº§n chÃ­nh

1. **Data Preparation** (`data/prepare_data.py`)
2. **Model Training** (`spark_jobs/train_model.py`)
3. **Streaming Prediction** (`spark_jobs/streaming_predict.py`)
4. **Kafka Producer** (`streaming/kafka_producer.py`)
5. **Kafka Consumer & Visualization** (`visualization/kafka_consumer.py`)
6. **Airflow Orchestration** (`dags/ml_pipeline_dag.py`)

---

## 3. PHÃ‚N TÃCH CHI TIáº¾T CÃC THÃ€NH PHáº¦N

### 3.1. Data Preparation (`data/prepare_data.py`)

#### 3.1.1. Má»¥c Ä‘Ã­ch
Chuáº©n bá»‹ vÃ  chia dá»¯ liá»‡u thÃ nh hai táº­p:
- **Training data**: 80% dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh
- **Streaming data**: 20% dá»¯ liá»‡u Ä‘á»ƒ mÃ´ phá»ng streaming

#### 3.1.2. Chá»©c nÄƒng chÃ­nh

```python
def prepare_data():
    # Táº£i dataset California Housing
    housing = fetch_california_housing()
    df = pd.DataFrame(housing.data, columns=housing.feature_names)
    df['target'] = housing.target
    
    # Chia 80% train, 20% streaming
    train_df, streaming_df = train_test_split(df, test_size=0.2, random_state=42)
    
    # LÆ°u vÃ o CSV
    train_df.to_csv('data/train_data.csv', index=False)
    streaming_df.to_csv('data/streaming_data.csv', index=False)
```

#### 3.1.3. Äáº·c Ä‘iá»ƒm
- Sá»­ dá»¥ng `random_state=42` Ä‘á»ƒ Ä‘áº£m báº£o reproducibility
- Tá»± Ä‘á»™ng táº¡o thÆ° má»¥c `data/` náº¿u chÆ°a tá»“n táº¡i
- Táº¡o file `README.md` mÃ´ táº£ dataset

#### 3.1.4. Káº¿t quáº£
- `data/train_data.csv`: ~16,512 máº«u (80%)
- `data/streaming_data.csv`: ~4,128 máº«u (20%)

---

### 3.2. Model Training (`spark_jobs/train_model.py`)

#### 3.2.1. Má»¥c Ä‘Ã­ch
Huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest Regressor Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  sá»­ dá»¥ng Spark ML.

#### 3.2.2. Kiáº¿n trÃºc mÃ´ hÃ¬nh

**Pipeline gá»“m 2 stages:**
1. **VectorAssembler**: Káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng thÃ nh vector
2. **RandomForestRegressor**: MÃ´ hÃ¬nh há»“i quy Random Forest

**Tham sá»‘ mÃ´ hÃ¬nh:**
- `numTrees`: 100 cÃ¢y
- `maxDepth`: 10
- `seed`: 42 (Ä‘áº£m báº£o reproducibility)

#### 3.2.3. Quy trÃ¬nh huáº¥n luyá»‡n

```python
# 1. Khá»Ÿi táº¡o Spark Session
spark = SparkSession.builder \
    .appName("HousePriceModelTraining") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

# 2. Äá»c dá»¯ liá»‡u
df = spark.read.csv("file://data/train_data.csv", header=True, inferSchema=True)

# 3. Táº¡o pipeline
pipeline = Pipeline(stages=[assembler, rf])

# 4. Chia train/test (80/20)
train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)

# 5. Huáº¥n luyá»‡n
model = pipeline.fit(train_data)

# 6. ÄÃ¡nh giÃ¡
predictions = model.transform(test_data)
```

#### 3.2.4. Metrics Ä‘Ã¡nh giÃ¡

MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng 3 metrics:
- **RMSE** (Root Mean Squared Error): Sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh
- **MAE** (Mean Absolute Error): Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh
- **RÂ²** (Coefficient of Determination): Há»‡ sá»‘ xÃ¡c Ä‘á»‹nh

#### 3.2.5. LÆ°u trá»¯ mÃ´ hÃ¬nh

MÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u vÃ o: `models/house_price_model/`

Cáº¥u trÃºc thÆ° má»¥c:
```
models/house_price_model/
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ part-00000-*.txt
â””â”€â”€ stages/
    â”œâ”€â”€ 0_VectorAssembler_*/
    â””â”€â”€ 1_RandomForestRegressor_*/
        â”œâ”€â”€ data/
        â”œâ”€â”€ metadata/
        â””â”€â”€ treesMetadata/
```

#### 3.2.6. Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t
- Sá»­ dá»¥ng Spark ML Pipeline Ä‘á»ƒ dá»… dÃ ng triá»ƒn khai
- Há»— trá»£ distributed training trÃªn Spark cluster
- Tá»± Ä‘á»™ng infer schema tá»« CSV
- Memory configuration: 4GB driver, 4GB executor

---

### 3.3. Streaming Prediction (`spark_jobs/streaming_predict.py`)

#### 3.3.1. Má»¥c Ä‘Ã­ch
Äá»c dá»¯ liá»‡u streaming tá»« Kafka, Ã¡p dá»¥ng mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n, vÃ  gá»­i káº¿t quáº£ vá» Kafka topic khÃ¡c.

#### 3.3.2. Kiáº¿n trÃºc Spark Streaming

**Input:**
- Kafka topic: `house-prices-input`
- Format: JSON vá»›i schema Ä‘á»‹nh nghÄ©a sáºµn

**Output:**
- Kafka topic: `house-prices-output`
- Format: JSON chá»©a id, actual_price, predicted_price, error, error_percentage

#### 3.3.3. Schema dá»¯ liá»‡u

```python
schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("MedInc", DoubleType(), True),
    StructField("HouseAge", DoubleType(), True),
    StructField("AveRooms", DoubleType(), True),
    StructField("AveBedrms", DoubleType(), True),
    StructField("Population", DoubleType(), True),
    StructField("AveOccup", DoubleType(), True),
    StructField("Latitude", DoubleType(), True),
    StructField("Longitude", DoubleType(), True),
    StructField("actual_price", DoubleType(), True)
])
```

#### 3.3.4. Quy trÃ¬nh xá»­ lÃ½

```python
# 1. Load mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
model = PipelineModel.load("models/house_price_model")

# 2. Äá»c stream tá»« Kafka
df_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "192.168.80.127:9092") \
    .option("subscribe", "house-prices-input") \
    .option("startingOffsets", "earliest") \
    .load()

# 3. Parse JSON
df_parsed = df_stream.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# 4. Dá»± Ä‘oÃ¡n
predictions = model.transform(df_parsed)

# 5. TÃ­nh toÃ¡n metrics
result = predictions.select(
    col("id"),
    col("actual_price"),
    col("prediction").alias("predicted_price"),
    (col("prediction") - col("actual_price")).alias("error"),
    ((col("prediction") - col("actual_price")) / col("actual_price") * 100).alias("error_percentage")
)

# 6. Gá»­i káº¿t quáº£ vá» Kafka
kafka_output = result.select(to_json(struct("*")).alias("value"))

query = kafka_output.writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "192.168.80.127:9092") \
    .option("topic", "house-prices-output") \
    .option("checkpointLocation", "/tmp/checkpoint-house-prices-output") \
    .start()
```

#### 3.3.5. TÃ­nh nÄƒng Ä‘áº·c biá»‡t

**Timeout mechanism:**
- Streaming job tá»± Ä‘á»™ng dá»«ng sau 120 giÃ¢y (2 phÃºt)
- Äáº£m báº£o job khÃ´ng cháº¡y vÃ´ háº¡n trong mÃ´i trÆ°á»ng production

**Checkpoint:**
- Sá»­ dá»¥ng checkpoint Ä‘á»ƒ Ä‘áº£m báº£o exactly-once semantics
- LÆ°u táº¡i `/tmp/checkpoint-house-prices-output`

**Console output:**
- Hiá»ƒn thá»‹ káº¿t quáº£ dá»± Ä‘oÃ¡n trÃªn console Ä‘á»ƒ debug
- Format: append mode vá»›i truncate=False

#### 3.3.6. Cáº¥u hÃ¬nh Kafka

- **Bootstrap servers**: `192.168.80.127:9092`
- **Starting offsets**: `earliest` (Ä‘á»c tá»« Ä‘áº§u náº¿u chÆ°a cÃ³ offset)
- **Fail on data loss**: `false` (khÃ´ng fail náº¿u offset bá»‹ reset)

#### 3.3.7. Dependencies

- `org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0`: Kafka connector cho Spark

---

### 3.4. Kafka Producer (`streaming/kafka_producer.py`)

#### 3.4.1. Má»¥c Ä‘Ã­ch
MÃ´ phá»ng nguá»“n dá»¯ liá»‡u streaming báº±ng cÃ¡ch gá»­i dá»¯ liá»‡u tá»« file CSV vÃ o Kafka topic.

#### 3.4.2. Chá»©c nÄƒng chÃ­nh

```python
def send_streaming_data(interval=2, num_records=None):
    # Äá»c dá»¯ liá»‡u streaming
    df = pd.read_csv('data/streaming_data.csv')
    
    if num_records:
        df = df.head(num_records)
    
    # Táº¡o producer
    producer = create_producer()
    
    # Gá»­i tá»«ng record
    for idx, row in df.iterrows():
        message = {
            'id': idx,
            'MedInc': float(row['MedInc']),
            # ... cÃ¡c trÆ°á»ng khÃ¡c
            'actual_price': float(row['target'])
        }
        producer.send('house-prices-input', value=message)
        time.sleep(interval)
```

#### 3.4.3. TÃ­nh nÄƒng

**Retry logic:**
- Tá»± Ä‘á»™ng retry káº¿t ná»‘i Ä‘áº¿n Kafka náº¿u chÆ°a sáºµn sÃ ng
- Max retries: 10 láº§n vá»›i delay 5 giÃ¢y

**Tham sá»‘ dÃ²ng lá»‡nh:**
- `interval`: Khoáº£ng thá»i gian giá»¯a cÃ¡c message (máº·c Ä‘á»‹nh: 2 giÃ¢y)
- `num_records`: Sá»‘ lÆ°á»£ng records gá»­i (máº·c Ä‘á»‹nh: táº¥t cáº£)

**Message format:**
- JSON serialization vá»›i UTF-8 encoding
- Kafka API version: (2, 5, 0)

#### 3.4.4. Cáº¥u hÃ¬nh

- **Bootstrap servers**: `192.168.80.127:9092`
- **Topic**: `house-prices-input`
- **Value serializer**: JSON dumps vá»›i UTF-8 encoding

#### 3.4.5. Output

In ra console thÃ´ng tin má»—i message Ä‘Ã£ gá»­i:
```
ğŸ“¤ ÄÃ£ gá»­i báº£n ghi 1/200 | MedInc=8.32 | Actual Price=$452.60K
```

---

### 3.5. Kafka Consumer & Visualization (`visualization/kafka_consumer.py`)

#### 3.5.1. Má»¥c Ä‘Ã­ch
Äá»c káº¿t quáº£ dá»± Ä‘oÃ¡n tá»« Kafka vÃ  hiá»ƒn thá»‹ trá»±c quan hÃ³a real-time.

#### 3.5.2. Kiáº¿n trÃºc Visualization

**Class: `RealtimeVisualizer`**

**ThÃ nh pháº§n:**
- **Data structures**: Sá»­ dá»¥ng `deque` vá»›i max length Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u
- **Matplotlib**: Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ vá»›i animation
- **Kafka Consumer**: Äá»c tá»« topic `house-prices-output`

#### 3.5.3. Biá»ƒu Ä‘á»“ hiá»ƒn thá»‹

**Plot 1: Actual vs Predicted Prices**
- Line chart so sÃ¡nh giÃ¡ thá»±c táº¿ vÃ  giÃ¡ dá»± Ä‘oÃ¡n
- X-axis: Sample Index
- Y-axis: Price ($1000s)
- Legend: Actual Price (blue), Predicted Price (red)

**Plot 2: Prediction Error**
- Bar chart hiá»ƒn thá»‹ sai sá»‘ tuyá»‡t Ä‘á»‘i
- X-axis: Sample Index
- Y-axis: Absolute Error ($1000s)
- Color: Coral vá»›i alpha=0.7

**Metrics display:**
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)
- Sá»‘ lÆ°á»£ng samples Ä‘Ã£ xá»­ lÃ½

#### 3.5.4. Quy trÃ¬nh hoáº¡t Ä‘á»™ng

```python
# 1. Khá»Ÿi táº¡o consumer
consumer = KafkaConsumer(
    'house-prices-output',
    bootstrap_servers=['192.168.80.127:9092'],
    group_id='viz-consumer',
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

# 2. Poll messages
messages = consumer.poll(timeout_ms=100)

# 3. Update data structures
for record in records:
    data = record.value
    self.ids.append(data['id'])
    self.actual_prices.append(data['actual_price'] * 100)
    self.predicted_prices.append(data['predicted_price'] * 100)
    self.errors.append(abs(data['error']) * 100)

# 4. Update plot vá»›i FuncAnimation
ani = FuncAnimation(self.fig, self.update_plot, interval=1000)
```

#### 3.5.5. TÃ­nh nÄƒng

**Real-time updates:**
- Cáº­p nháº­t biá»ƒu Ä‘á»“ má»—i 1 giÃ¢y
- Sá»­ dá»¥ng `FuncAnimation` tá»« matplotlib

**Data management:**
- Giá»›i háº¡n sá»‘ Ä‘iá»ƒm hiá»ƒn thá»‹ (max_points=100)
- Sá»­ dá»¥ng deque Ä‘á»ƒ tá»± Ä‘á»™ng loáº¡i bá» dá»¯ liá»‡u cÅ©

**Consumer group:**
- Group ID: `viz-consumer`
- Auto commit: True
- Auto offset reset: `earliest`

#### 3.5.6. Output

Console output cho má»—i message:
```
ğŸ“Š ID:    1 | Actual: $452.60K | Predicted: $445.23K | Error:   1.63%
```

---

### 3.6. Airflow Orchestration (`dags/ml_pipeline_dag.py`)

#### 3.6.1. Má»¥c Ä‘Ã­ch
Äiá»u phá»‘i vÃ  tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ pipeline tá»« chuáº©n bá»‹ dá»¯ liá»‡u Ä‘áº¿n visualization.

#### 3.6.2. Cáº¥u hÃ¬nh há»‡ thá»‘ng phÃ¢n tÃ¡n

```python
KAFKA_HOST, KAFKA_PORT, KAFKA_USER = "192.168.80.127", 9092, "nindang"
SPARK_HOST, SPARK_USER = "192.168.80.207", "nindang"
SPARK_MASTER = f"spark://{SPARK_HOST}:7077"
PROJECT_DIR = "/home/haminhchien/Documents/bigdata/final_project"
```

#### 3.6.3. DAG chÃ­nh: `ml_streaming_pipeline_distributed`

**CÃ¡c tasks:**

1. **start_kafka_remote**
   - Khá»Ÿi Ä‘á»™ng Kafka cluster trÃªn mÃ¡y remote
   - Sá»­ dá»¥ng SSH Ä‘á»ƒ cháº¡y docker-compose
   - Kiá»ƒm tra containers Ä‘Ã£ cháº¡y

2. **check_kafka_remote**
   - Kiá»ƒm tra Kafka Ä‘Ã£ sáºµn sÃ ng
   - Sá»­ dá»¥ng socket connection test
   - Max retries: 30 vá»›i delay 10 giÃ¢y

3. **ensure_kafka_output_topic**
   - Táº¡o Kafka topic `house-prices-output` náº¿u chÆ°a tá»“n táº¡i
   - Replication factor: 1
   - Partitions: 1

4. **start_spark_remote**
   - Khá»Ÿi Ä‘á»™ng Spark Master vÃ  Worker trÃªn mÃ¡y remote
   - Sá»­ dá»¥ng SSH Ä‘á»ƒ cháº¡y Spark scripts
   - Kiá»ƒm tra processes Ä‘Ã£ cháº¡y

5. **check_spark_remote**
   - Kiá»ƒm tra Spark Master Ä‘Ã£ sáºµn sÃ ng
   - Port: 7077
   - Max retries: 10 vá»›i delay 5 giÃ¢y

6. **prepare_data**
   - Cháº¡y script `data/prepare_data.py`
   - Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn

7. **train_model**
   - Submit Spark job Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh
   - Cáº¥u hÃ¬nh: 4GB driver, 4GB executor, 2 executors, 2 cores/executor

8. **send_data_to_remote_kafka**
   - Cháº¡y Kafka producer Ä‘á»ƒ gá»­i dá»¯ liá»‡u
   - Tham sá»‘: interval=1, num_records=200

9. **start_streaming_job**
   - Khá»Ÿi Ä‘á»™ng Spark Streaming job
   - XÃ³a checkpoint cÅ© Ä‘á»ƒ Ä‘á»c láº¡i tá»« Ä‘áº§u
   - Timeout: 5 phÃºt

10. **wait_for_streaming**
    - Äá»£i streaming xá»­ lÃ½ hoÃ n thÃ nh
    - Sleep: 5 phÃºt (300 giÃ¢y)

11. **cleanup**
    - Dá»n dáº¹p processes vÃ  checkpoints
    - Trigger rule: `all_done` (cháº¡y dÃ¹ thÃ nh cÃ´ng hay tháº¥t báº¡i)

#### 3.6.4. Dependencies giá»¯a cÃ¡c tasks

```
start_kafka_remote >> check_kafka >> ensure_kafka_output_topic
start_spark_remote >> check_spark
[ensure_kafka_output_topic, check_spark] >> prepare_data >> train_model >> send_streaming_data >> start_streaming_job >> wait_processing >> cleanup
```

#### 3.6.5. DAG Visualization: `ml_streaming_visualization`

**Task:**
- `run_visualization`: Cháº¡y script `visualization/kafka_consumer.py`

#### 3.6.6. Helper functions

**check_remote_ready(host, port, name, max_retries, delay)**
- Kiá»ƒm tra service Ä‘Ã£ sáºµn sÃ ng báº±ng socket connection
- Retry logic vá»›i delay

**wait_for_streaming_complete()**
- Äá»£i streaming hoÃ n thÃ nh vá»›i timeout

#### 3.6.7. Default arguments

```python
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
```

#### 3.6.8. Äáº·c Ä‘iá»ƒm

- **Schedule**: None (manual trigger)
- **Catchup**: False
- **Tags**: ['distributed', 'machine-learning', 'kafka', 'spark']
- **SSH-based execution**: Sá»­ dá»¥ng SSH Ä‘á»ƒ cháº¡y commands trÃªn mÃ¡y remote
- **Error handling**: Retry logic vÃ  cleanup tasks

---

## 4. LUá»’NG Xá»¬ LÃ Dá»® LIá»†U

### 4.1. Luá»“ng tá»•ng quan

```
1. Airflow DAG Ä‘Æ°á»£c trigger
   â†“
2. Khá»Ÿi Ä‘á»™ng Kafka cluster (Machine 2)
   â†“
3. Khá»Ÿi Ä‘á»™ng Spark cluster (Machine 3)
   â†“
4. Chuáº©n bá»‹ dá»¯ liá»‡u (chia train/streaming)
   â†“
5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest
   â†“
6. LÆ°u mÃ´ hÃ¬nh vÃ o models/house_price_model/
   â†“
7. Khá»Ÿi Ä‘á»™ng Spark Streaming job
   â†“
8. Producer gá»­i dá»¯ liá»‡u vÃ o Kafka topic: house-prices-input
   â†“
9. Spark Streaming Ä‘á»c tá»« Kafka, dá»± Ä‘oÃ¡n, gá»­i káº¿t quáº£ vÃ o house-prices-output
   â†“
10. Consumer Ä‘á»c káº¿t quáº£ vÃ  hiá»ƒn thá»‹ visualization
```

### 4.2. Luá»“ng dá»¯ liá»‡u chi tiáº¿t

**Stage 1: Data Preparation**
```
California Housing Dataset
    â†“
prepare_data.py
    â†“
train_data.csv (80%)    streaming_data.csv (20%)
```

**Stage 2: Model Training**
```
train_data.csv
    â†“
Spark ML Pipeline
    â”œâ”€â”€ VectorAssembler
    â””â”€â”€ RandomForestRegressor
    â†“
Trained Model (models/house_price_model/)
```

**Stage 3: Streaming Prediction**
```
streaming_data.csv
    â†“
kafka_producer.py
    â†“
Kafka Topic: house-prices-input
    â†“
Spark Streaming (streaming_predict.py)
    â”œâ”€â”€ Load Model
    â”œâ”€â”€ Parse JSON
    â”œâ”€â”€ Predict
    â””â”€â”€ Calculate Metrics
    â†“
Kafka Topic: house-prices-output
    â†“
kafka_consumer.py
    â†“
Real-time Visualization
```

### 4.3. Data formats

**Input to Kafka (house-prices-input):**
```json
{
  "id": 0,
  "MedInc": 8.3252,
  "HouseAge": 41.0,
  "AveRooms": 6.984127,
  "AveBedrms": 1.023810,
  "Population": 322.0,
  "AveOccup": 2.555556,
  "Latitude": 37.88,
  "Longitude": -122.23,
  "actual_price": 4.526
}
```

**Output from Kafka (house-prices-output):**
```json
{
  "id": 0,
  "actual_price": 4.526,
  "predicted_price": 4.4523,
  "error": -0.0737,
  "error_percentage": -1.63
}
```

---

## 5. Káº¾T QUáº¢ VÃ€ ÄÃNH GIÃ

### 5.1. Metrics mÃ´ hÃ¬nh

Sau khi huáº¥n luyá»‡n, mÃ´ hÃ¬nh Random Forest Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng cÃ¡c metrics:
- **RMSE**: Root Mean Squared Error (cÃ ng tháº¥p cÃ ng tá»‘t)
- **MAE**: Mean Absolute Error (cÃ ng tháº¥p cÃ ng tá»‘t)
- **RÂ²**: Coefficient of Determination (cÃ ng gáº§n 1 cÃ ng tá»‘t)

### 5.2. Streaming performance

**Throughput:**
- Producer gá»­i vá»›i interval 1-2 giÃ¢y/record
- Spark Streaming xá»­ lÃ½ real-time vá»›i latency tháº¥p
- Consumer hiá»ƒn thá»‹ káº¿t quáº£ vá»›i update rate 1 giÃ¢y

**Scalability:**
- Há»‡ thá»‘ng há»— trá»£ distributed processing
- Spark cluster cÃ³ thá»ƒ scale thÃªm workers
- Kafka há»— trá»£ multiple partitions vÃ  replication

### 5.3. Visualization insights

**Biá»ƒu Ä‘á»“ hiá»ƒn thá»‹:**
- So sÃ¡nh trá»±c quan giá»¯a giÃ¡ thá»±c táº¿ vÃ  giÃ¡ dá»± Ä‘oÃ¡n
- PhÃ¢n tÃ­ch sai sá»‘ dá»± Ä‘oÃ¡n theo tá»«ng sample
- Metrics tá»•ng há»£p (MAE, RMSE) Ä‘Æ°á»£c cáº­p nháº­t real-time

### 5.4. Äá»™ tin cáº­y há»‡ thá»‘ng

**Fault tolerance:**
- Kafka checkpoint Ä‘áº£m báº£o exactly-once semantics
- Spark Streaming cÃ³ kháº£ nÄƒng recover tá»« checkpoint
- Airflow retry logic cho cÃ¡c tasks

**Error handling:**
- Producer cÃ³ retry logic khi Kafka chÆ°a sáºµn sÃ ng
- Spark Streaming cÃ³ failOnDataLoss=false Ä‘á»ƒ trÃ¡nh crash
- Airflow cÃ³ cleanup task Ä‘á»ƒ dá»n dáº¹p resources

---

## 6. Káº¾T LUáº¬N

### 6.1. ThÃ nh tá»±u Ä‘áº¡t Ä‘Æ°á»£c

1. **XÃ¢y dá»±ng thÃ nh cÃ´ng pipeline ML streaming end-to-end**
   - Tá»« chuáº©n bá»‹ dá»¯ liá»‡u Ä‘áº¿n visualization
   - Tá»± Ä‘á»™ng hÃ³a hoÃ n toÃ n vá»›i Airflow

2. **Triá»ƒn khai trÃªn kiáº¿n trÃºc phÃ¢n tÃ¡n**
   - 3 mÃ¡y riÃªng biá»‡t cho cÃ¡c services
   - Distributed processing vá»›i Spark cluster

3. **Real-time prediction**
   - Streaming data processing vá»›i Spark Structured Streaming
   - Low latency prediction pipeline

4. **Visualization real-time**
   - Biá»ƒu Ä‘á»“ cáº­p nháº­t liÃªn tá»¥c
   - Metrics Ä‘Æ°á»£c tÃ­nh toÃ¡n vÃ  hiá»ƒn thá»‹ Ä‘á»™ng

### 6.2. CÃ´ng nghá»‡ sá»­ dá»¥ng

- **Apache Spark 4.0.0**: Distributed computing vÃ  ML
- **Apache Kafka**: Message queue cho streaming
- **Apache Airflow**: Workflow orchestration
- **Python**: NgÃ´n ngá»¯ láº­p trÃ¬nh chÃ­nh
- **Matplotlib**: Visualization
- **Docker**: Containerization cho Kafka

### 6.3. á»¨ng dá»¥ng thá»±c táº¿

Há»‡ thá»‘ng nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho:
- **Real-time price prediction**: Dá»± Ä‘oÃ¡n giÃ¡ nhÃ , giÃ¡ cá»• phiáº¿u
- **IoT data processing**: Xá»­ lÃ½ dá»¯ liá»‡u tá»« sensors
- **Recommendation systems**: Há»‡ thá»‘ng gá»£i Ã½ real-time
- **Fraud detection**: PhÃ¡t hiá»‡n gian láº­n trong giao dá»‹ch

### 6.4. Háº¡n cháº¿ vÃ  cáº£i thiá»‡n

**Háº¡n cháº¿ hiá»‡n táº¡i:**
- Model Ä‘Æ°á»£c huáº¥n luyá»‡n offline, chÆ°a cÃ³ online learning
- Visualization chá»‰ hiá»ƒn thá»‹ trÃªn local machine
- ChÆ°a cÃ³ monitoring vÃ  alerting system

**HÆ°á»›ng cáº£i thiá»‡n:**
- ThÃªm model versioning vÃ  A/B testing
- Triá»ƒn khai visualization trÃªn web dashboard
- TÃ­ch há»£p monitoring tools (Prometheus, Grafana)
- ThÃªm data validation vÃ  quality checks
- Implement model retraining pipeline tá»± Ä‘á»™ng

### 6.5. Káº¿t luáº­n

Dá»± Ã¡n Ä‘Ã£ thÃ nh cÃ´ng xÃ¢y dá»±ng má»™t há»‡ thá»‘ng Machine Learning streaming hoÃ n chá»‰nh vá»›i cÃ¡c cÃ´ng nghá»‡ Big Data hiá»‡n Ä‘áº¡i. Há»‡ thá»‘ng cÃ³ kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u real-time, scale Ä‘Æ°á»£c vÃ  cÃ³ Ä‘á»™ tin cáº­y cao. ÄÃ¢y lÃ  má»™t foundation tá»‘t Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c á»©ng dá»¥ng ML production-ready.

---

## PHá»¤ Lá»¤C

### A. Cáº¥u trÃºc thÆ° má»¥c dá»± Ã¡n

```
final_project/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ml_pipeline_dag.py          # Airflow DAG Ä‘iá»u khiá»ƒn toÃ n bá»™
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prepare_data.py              # Chia dá»¯ liá»‡u train/streaming
â”‚   â”œâ”€â”€ train_data.csv               # Dá»¯ liá»‡u huáº¥n luyá»‡n
â”‚   â””â”€â”€ streaming_data.csv          # Dá»¯ liá»‡u streaming
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # Kafka + Zookeeper (tham kháº£o)
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ train_model.py               # Huáº¥n luyá»‡n mÃ´ hÃ¬nh Spark ML
â”‚   â””â”€â”€ streaming_predict.py        # Spark Streaming dá»± Ä‘oÃ¡n
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ kafka_producer.py            # MÃ´ phá»ng streaming vÃ o Kafka
â”œâ”€â”€ visualization/
â”‚   â””â”€â”€ kafka_consumer.py            # Trá»±c quan hÃ³a káº¿t quáº£
â”œâ”€â”€ models/
â”‚   â””â”€â”€ house_price_model/           # MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # HÆ°á»›ng dáº«n sá»­ dá»¥ng
```

### B. Dependencies chÃ­nh

**Python packages:**
- `pyspark==4.0.0`
- `kafka-python`
- `pandas`
- `scikit-learn`
- `matplotlib`
- `apache-airflow==2.7.0`

**Spark packages:**
- `org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0`

### C. Cáº¥u hÃ¬nh há»‡ thá»‘ng

**Kafka:**
- Bootstrap servers: `192.168.80.127:9092`
- Topics: `house-prices-input`, `house-prices-output`

**Spark:**
- Master: `spark://192.168.80.207:7077`
- Driver memory: 4GB
- Executor memory: 4GB
- Executors: 2
- Cores per executor: 2

**Airflow:**
- Web server port: 8080
- Default user: admin/admin

---

**TÃ i liá»‡u Ä‘Æ°á»£c táº¡o bá»Ÿi:** Há»‡ thá»‘ng phÃ¢n tÃ­ch tá»± Ä‘á»™ng  
**NgÃ y táº¡o:** 2025-12-15  
**PhiÃªn báº£n:** 1.0

